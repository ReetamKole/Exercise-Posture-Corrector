{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import gradio as gr\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Drawing helpers\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine important landmarks for plank\n",
    "IMPORTANT_LMS = [\n",
    "    \"NOSE\",\n",
    "    \"LEFT_SHOULDER\",\n",
    "    \"RIGHT_SHOULDER\",\n",
    "    \"RIGHT_ELBOW\",\n",
    "    \"LEFT_ELBOW\",\n",
    "    \"RIGHT_WRIST\",\n",
    "    \"LEFT_WRIST\",\n",
    "    \"LEFT_HIP\",\n",
    "    \"RIGHT_HIP\",\n",
    "]\n",
    "\n",
    "# Generate all columns of the data frame\n",
    "\n",
    "HEADERS = [\"label\"] # Label column\n",
    "\n",
    "for lm in IMPORTANT_LMS:\n",
    "    HEADERS += [f\"{lm.lower()}_x\", f\"{lm.lower()}_y\", f\"{lm.lower()}_z\", f\"{lm.lower()}_v\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_frame(frame, percent=50):\n",
    "    '''\n",
    "    Rescale a frame from OpenCV to a certain percentage compare to its original frame\n",
    "    '''\n",
    "    width = int(frame.shape[1] * percent/ 100)\n",
    "    height = int(frame.shape[0] * percent/ 100)\n",
    "    dim = (width, height)\n",
    "    return cv2.resize(frame, dim, interpolation =cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "def save_frame_as_image(frame, message: str = None):\n",
    "    '''\n",
    "    Save a frame as image to display the error\n",
    "    '''\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    if message:\n",
    "        cv2.putText(frame, message, (50, 150), cv2.FONT_HERSHEY_COMPLEX, 0.4, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        \n",
    "    print(\"Saving ...\")\n",
    "    cv2.imwrite(f\"C:/SRM/sem5/ml/project/Exercise-Correction/core/bicep_model/bicep_{now}.jpg\", frame)\n",
    "\n",
    "\n",
    "def calculate_angle(point1: list, point2: list, point3: list) -> float:\n",
    "    '''\n",
    "    Calculate the angle between 3 points\n",
    "    Unit of the angle will be in Degree\n",
    "    '''\n",
    "    point1 = np.array(point1)\n",
    "    point2 = np.array(point2)\n",
    "    point3 = np.array(point3)\n",
    "\n",
    "    # Calculate algo\n",
    "    angleInRad = np.arctan2(point3[1] - point2[1], point3[0] - point2[0]) - np.arctan2(point1[1] - point2[1], point1[0] - point2[0])\n",
    "    angleInDeg = np.abs(angleInRad * 180.0 / np.pi)\n",
    "\n",
    "    angleInDeg = angleInDeg if angleInDeg <= 180 else 360 - angleInDeg\n",
    "    return angleInDeg\n",
    "\n",
    "\n",
    "def extract_important_keypoints(results, important_landmarks: list) -> list:\n",
    "    '''\n",
    "    Extract important keypoints from mediapipe pose detection\n",
    "    '''\n",
    "    landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "    data = []\n",
    "    for lm in important_landmarks:\n",
    "        keypoint = landmarks[mp_pose.PoseLandmark[lm].value]\n",
    "        data.append([keypoint.x, keypoint.y, keypoint.z, keypoint.visibility])\n",
    "    \n",
    "    return np.array(data).flatten().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BicepPoseAnalysis:\n",
    "    def __init__(self, side: str, stage_down_threshold: float, stage_up_threshold: float, peak_contraction_threshold: float, loose_upper_arm_angle_threshold: float, visibility_threshold: float):\n",
    "        # Initialize thresholds\n",
    "        self.stage_down_threshold = stage_down_threshold\n",
    "        self.stage_up_threshold = stage_up_threshold\n",
    "        self.peak_contraction_threshold = peak_contraction_threshold\n",
    "        self.loose_upper_arm_angle_threshold = loose_upper_arm_angle_threshold\n",
    "        self.visibility_threshold = visibility_threshold\n",
    "\n",
    "        self.side = side\n",
    "        self.counter = 0\n",
    "        self.stage = \"down\"\n",
    "        self.is_visible = True\n",
    "        self.detected_errors = {\n",
    "            \"LOOSE_UPPER_ARM\": 0,\n",
    "            \"PEAK_CONTRACTION\": 0,\n",
    "        }\n",
    "\n",
    "        # Params for loose upper arm error detection\n",
    "        self.loose_upper_arm = False\n",
    "\n",
    "        # Params for peak contraction error detection\n",
    "        self.peak_contraction_angle = 1000\n",
    "        self.peak_contraction_frame = None\n",
    "    \n",
    "    def get_joints(self, landmarks) -> bool:\n",
    "        '''\n",
    "        Check for joints' visibility then get joints coordinate\n",
    "        '''\n",
    "        side = self.side.upper()\n",
    "\n",
    "        # Check visibility\n",
    "        joints_visibility = [ landmarks[mp_pose.PoseLandmark[f\"{side}_SHOULDER\"].value].visibility, landmarks[mp_pose.PoseLandmark[f\"{side}_ELBOW\"].value].visibility, landmarks[mp_pose.PoseLandmark[f\"{side}_WRIST\"].value].visibility ]\n",
    "\n",
    "        is_visible = all([ vis > self.visibility_threshold for vis in joints_visibility ])\n",
    "        self.is_visible = is_visible\n",
    "\n",
    "        if not is_visible:\n",
    "            return self.is_visible\n",
    "        \n",
    "        # Get joints' coordinates\n",
    "        self.shoulder = [ landmarks[mp_pose.PoseLandmark[f\"{side}_SHOULDER\"].value].x, landmarks[mp_pose.PoseLandmark[f\"{side}_SHOULDER\"].value].y ]\n",
    "        self.elbow = [ landmarks[mp_pose.PoseLandmark[f\"{side}_ELBOW\"].value].x, landmarks[mp_pose.PoseLandmark[f\"{side}_ELBOW\"].value].y ]\n",
    "        self.wrist = [ landmarks[mp_pose.PoseLandmark[f\"{side}_WRIST\"].value].x, landmarks[mp_pose.PoseLandmark[f\"{side}_WRIST\"].value].y ]\n",
    "\n",
    "        return self.is_visible\n",
    "    \n",
    "    def analyze_pose(self, landmarks, frame):\n",
    "        '''\n",
    "        - Bicep Counter\n",
    "        - Errors Detection\n",
    "        '''\n",
    "        self.get_joints(landmarks)\n",
    "\n",
    "        # Cancel calculation if visibility is poor\n",
    "        if not self.is_visible:\n",
    "            return (None, None)\n",
    "\n",
    "        # * Calculate curl angle for counter\n",
    "        bicep_curl_angle = int(calculate_angle(self.shoulder, self.elbow, self.wrist))\n",
    "        if bicep_curl_angle > self.stage_down_threshold:\n",
    "            self.stage = \"down\"\n",
    "        elif bicep_curl_angle < self.stage_up_threshold and self.stage == \"down\":\n",
    "            self.stage = \"up\"\n",
    "            self.counter += 1\n",
    "        \n",
    "        # * Calculate the angle between the upper arm (shoulder & joint) and the Y axis\n",
    "        shoulder_projection = [ self.shoulder[0], 1 ] # Represent the projection of the shoulder to the X axis\n",
    "        ground_upper_arm_angle = int(calculate_angle(self.elbow, self.shoulder, shoulder_projection))\n",
    "\n",
    "        # * Evaluation for LOOSE UPPER ARM error\n",
    "        if ground_upper_arm_angle > self.loose_upper_arm_angle_threshold:\n",
    "            # Limit the saved frame\n",
    "            if not self.loose_upper_arm:\n",
    "                self.loose_upper_arm = True\n",
    "                # save_frame_as_image(frame, f\"Loose upper arm: {ground_upper_arm_angle}\")\n",
    "                self.detected_errors[\"LOOSE_UPPER_ARM\"] += 1\n",
    "        else:\n",
    "            self.loose_upper_arm = False\n",
    "        \n",
    "        # * Evaluate PEAK CONTRACTION error\n",
    "        if self.stage == \"up\" and bicep_curl_angle < self.peak_contraction_angle:\n",
    "            # Save peaked contraction every rep\n",
    "            self.peak_contraction_angle = bicep_curl_angle\n",
    "            self.peak_contraction_frame = frame\n",
    "            \n",
    "        elif self.stage == \"down\":\n",
    "            # * Evaluate if the peak is higher than the threshold if True, marked as an error then saved that frame\n",
    "            if self.peak_contraction_angle != 1000 and self.peak_contraction_angle >= self.peak_contraction_threshold:\n",
    "                # save_frame_as_image(self.peak_contraction_frame, f\"{self.side} - Peak Contraction: {self.peak_contraction_angle}\")\n",
    "                self.detected_errors[\"PEAK_CONTRACTION\"] += 1\n",
    "            \n",
    "            # Reset params\n",
    "            self.peak_contraction_angle = 1000\n",
    "            self.peak_contraction_frame = None\n",
    "        \n",
    "        return (bicep_curl_angle, ground_upper_arm_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VIDEO_DEMO_PATH = \"C:/SRM/sem5/ml/project/Exercise-Correction/core/bicep_model/correct_bc1.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input scaler\n",
    "with open(\"C:/SRM/sem5/ml/project/Exercise-Correction/core/bicep_model/model/input_scaler.pkl\", \"rb\") as f:\n",
    "    input_scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "with open(\"C:/SRM/sem5/ml/project/Exercise-Correction/core/bicep_model/model/KNN_model.pkl\", \"rb\") as f:\n",
    "    sklearn_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(VIDEO_DEMO_PATH)\n",
    "\n",
    "VISIBILITY_THRESHOLD = 0.65\n",
    "\n",
    "# Params for counter\n",
    "STAGE_UP_THRESHOLD = 90\n",
    "STAGE_DOWN_THRESHOLD = 120\n",
    "\n",
    "# Params to catch FULL RANGE OF MOTION error\n",
    "PEAK_CONTRACTION_THRESHOLD = 60\n",
    "\n",
    "# LOOSE UPPER ARM error detection\n",
    "LOOSE_UPPER_ARM = False\n",
    "LOOSE_UPPER_ARM_ANGLE_THRESHOLD = 40\n",
    "\n",
    "# STANDING POSTURE error detection\n",
    "POSTURE_ERROR_THRESHOLD = 0.7\n",
    "posture = \"C\"\n",
    "\n",
    "# Init analysis class\n",
    "left_arm_analysis = BicepPoseAnalysis(side=\"left\", stage_down_threshold=STAGE_DOWN_THRESHOLD, stage_up_threshold=STAGE_UP_THRESHOLD, peak_contraction_threshold=PEAK_CONTRACTION_THRESHOLD, loose_upper_arm_angle_threshold=LOOSE_UPPER_ARM_ANGLE_THRESHOLD, visibility_threshold=VISIBILITY_THRESHOLD)\n",
    "\n",
    "right_arm_analysis = BicepPoseAnalysis(side=\"right\", stage_down_threshold=STAGE_DOWN_THRESHOLD, stage_up_threshold=STAGE_UP_THRESHOLD, peak_contraction_threshold=PEAK_CONTRACTION_THRESHOLD, loose_upper_arm_angle_threshold=LOOSE_UPPER_ARM_ANGLE_THRESHOLD, visibility_threshold=VISIBILITY_THRESHOLD)\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.8, min_tracking_confidence=0.8) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, image = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Reduce size of a frame\n",
    "        image = rescale_frame(image, 50)\n",
    "        # image = cv2.flip(image, 1)\n",
    "        \n",
    "        video_dimensions = [image.shape[1], image.shape[0]]\n",
    "\n",
    "        # Recolor image from BGR to RGB for mediapipe\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        results = pose.process(image)\n",
    "\n",
    "        if not results.pose_landmarks:\n",
    "            print(\"No human found\")\n",
    "            continue\n",
    "\n",
    "        # Recolor image from BGR to RGB for mediapipe\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw landmarks and connections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, mp_drawing.DrawingSpec(color=(244, 117, 66), thickness=2, circle_radius=2), mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=1))\n",
    "\n",
    "        # Make detection\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            (left_bicep_curl_angle, left_ground_upper_arm_angle) = left_arm_analysis.analyze_pose(landmarks=landmarks, frame=image)\n",
    "            (right_bicep_curl_angle, right_ground_upper_arm_angle) = right_arm_analysis.analyze_pose(landmarks=landmarks, frame=image)\n",
    "\n",
    "            # Extract keypoints from frame for the input\n",
    "            row = extract_important_keypoints(results, IMPORTANT_LMS)\n",
    "            X = pd.DataFrame([row], columns=HEADERS[1:])\n",
    "            X = pd.DataFrame(input_scaler.transform(X))\n",
    "\n",
    "\n",
    "            # Make prediction and its probability\n",
    "            predicted_class = sklearn_model.predict(X)[0]\n",
    "            prediction_probabilities = sklearn_model.predict_proba(X)[0]\n",
    "            class_prediction_probability = round(prediction_probabilities[np.argmax(prediction_probabilities)], 2)\n",
    "\n",
    "            if class_prediction_probability >= POSTURE_ERROR_THRESHOLD:\n",
    "                posture = predicted_class\n",
    "\n",
    "            # Visualization\n",
    "            # Status box\n",
    "            cv2.rectangle(image, (0, 0), (500, 40), (245, 117, 16), -1)\n",
    "\n",
    "            # Display probability\n",
    "            cv2.putText(image, \"RIGHT\", (15, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(right_arm_analysis.counter) if right_arm_analysis.is_visible else \"UNK\", (10, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Display Left Counter\n",
    "            cv2.putText(image, \"LEFT\", (95, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(left_arm_analysis.counter) if left_arm_analysis.is_visible else \"UNK\", (100, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # * Display error\n",
    "            # Right arm error\n",
    "            cv2.putText(image, \"R_PC\", (165, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(right_arm_analysis.detected_errors[\"PEAK_CONTRACTION\"]), (160, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, \"R_LUA\", (225, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(right_arm_analysis.detected_errors[\"LOOSE_UPPER_ARM\"]), (220, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Left arm error\n",
    "            cv2.putText(image, \"L_PC\", (300, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(left_arm_analysis.detected_errors[\"PEAK_CONTRACTION\"]), (295, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, \"L_LUA\", (380, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(left_arm_analysis.detected_errors[\"LOOSE_UPPER_ARM\"]), (375, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Lean back error\n",
    "            cv2.putText(image, \"LB\", (460, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(f\"{posture}, {predicted_class}, {class_prediction_probability}\"), (440, 30), cv2.FONT_HERSHEY_COMPLEX, 0.3, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "            # * Visualize angles\n",
    "            # Visualize LEFT arm calculated angles\n",
    "            if left_arm_analysis.is_visible:\n",
    "                cv2.putText(image, str(left_bicep_curl_angle), tuple(np.multiply(left_arm_analysis.elbow, video_dimensions).astype(int)), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "                cv2.putText(image, str(left_ground_upper_arm_angle), tuple(np.multiply(left_arm_analysis.shoulder, video_dimensions).astype(int)), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "            # Visualize RIGHT arm calculated angles\n",
    "            if right_arm_analysis.is_visible:\n",
    "                cv2.putText(image, str(right_bicep_curl_angle), tuple(np.multiply(right_arm_analysis.elbow, video_dimensions).astype(int)), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 0), 1, cv2.LINE_AA)\n",
    "                cv2.putText(image, str(right_ground_upper_arm_angle), tuple(np.multiply(right_arm_analysis.shoulder, video_dimensions).astype(int)), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 0), 1, cv2.LINE_AA)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        cv2.imshow(\"CV2\", image)\n",
    "\n",
    "        # if left_arm_analysis.loose_upper_arm:\n",
    "        #     save_frame_as_image(image, \"\")\n",
    "        \n",
    "        # Press Q to close cv2 window\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import mediapipe as mp\n",
    "\n",
    "# Import your BicepPoseAnalysis class and other required functions here\n",
    "\n",
    "# Initialize Mediapipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Define constants\n",
    "VISIBILITY_THRESHOLD = 0.65\n",
    "STAGE_UP_THRESHOLD = 90\n",
    "STAGE_DOWN_THRESHOLD = 120\n",
    "PEAK_CONTRACTION_THRESHOLD = 60\n",
    "LOOSE_UPPER_ARM_ANGLE_THRESHOLD = 40\n",
    "\n",
    "# Initialize pose analysis classes for left and right arm\n",
    "left_arm_analysis = BicepPoseAnalysis(side=\"left\", stage_down_threshold=STAGE_DOWN_THRESHOLD,\n",
    "                                       stage_up_threshold=STAGE_UP_THRESHOLD,\n",
    "                                       peak_contraction_threshold=PEAK_CONTRACTION_THRESHOLD,\n",
    "                                       loose_upper_arm_angle_threshold=LOOSE_UPPER_ARM_ANGLE_THRESHOLD,\n",
    "                                       visibility_threshold=VISIBILITY_THRESHOLD)\n",
    "\n",
    "right_arm_analysis = BicepPoseAnalysis(side=\"right\", stage_down_threshold=STAGE_DOWN_THRESHOLD,\n",
    "                                        stage_up_threshold=STAGE_UP_THRESHOLD,\n",
    "                                        peak_contraction_threshold=PEAK_CONTRACTION_THRESHOLD,\n",
    "                                        loose_upper_arm_angle_threshold=LOOSE_UPPER_ARM_ANGLE_THRESHOLD,\n",
    "                                        visibility_threshold=VISIBILITY_THRESHOLD)\n",
    "\n",
    "def analyze_pose(video):\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    results_list = []  # List to store results\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.8, min_tracking_confidence=0.8) as pose:\n",
    "        while cap.isOpened():\n",
    "            ret, image = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Process the image\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(image)\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                \n",
    "                (left_bicep_curl_angle, left_ground_upper_arm_angle) = left_arm_analysis.analyze_pose(landmarks=landmarks, frame=image)\n",
    "                (right_bicep_curl_angle, right_ground_upper_arm_angle) = right_arm_analysis.analyze_pose(landmarks=landmarks, frame=image)\n",
    "\n",
    "                # Prepare results for output as a single line string\n",
    "                result_string = f\"Left Angle: {left_bicep_curl_angle}, Right Angle: {right_bicep_curl_angle}, \" \\\n",
    "                                f\"Left Counter: {left_arm_analysis.counter}, Right Counter: {right_arm_analysis.counter}\"\n",
    "                \n",
    "                results_list.append(result_string)\n",
    "\n",
    "                # Draw landmarks\n",
    "                mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "            # Convert back to BGR for OpenCV\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # You can display the image in the Gradio app\n",
    "            cv2.imshow(\"Pose Analysis\", image)\n",
    "\n",
    "            # Press 'q' to quit the window if needed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return results_list\n",
    "\n",
    "# Define Gradio interface\n",
    "iface = gr.Interface(fn=analyze_pose,\n",
    "                     inputs=gr.Video(label=\"Upload a Video for Pose Analysis\"),\n",
    "                     outputs=gr.JSON(label=\"Analysis Results\"),\n",
    "                     title=\"Bicep Pose Analysis\",\n",
    "                     description=\"Upload a video to analyze bicep curl poses and see the results.\")\n",
    "\n",
    "# Launch Gradio app\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback _ProactorBasePipeTransport._call_connection_lost(None)\n",
      "handle: <Handle _ProactorBasePipeTransport._call_connection_lost(None)>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Reetam K\\Anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Reetam K\\Anaconda3\\Lib\\asyncio\\proactor_events.py\", line 165, in _call_connection_lost\n",
      "    self._sock.shutdown(socket.SHUT_RDWR)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import mediapipe as mp\n",
    "\n",
    "# Import your BicepPoseAnalysis class and other required functions here\n",
    "\n",
    "# Initialize Mediapipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Define constants\n",
    "VISIBILITY_THRESHOLD = 0.65\n",
    "STAGE_UP_THRESHOLD = 90\n",
    "STAGE_DOWN_THRESHOLD = 120\n",
    "PEAK_CONTRACTION_THRESHOLD = 60\n",
    "LOOSE_UPPER_ARM_ANGLE_THRESHOLD = 40\n",
    "\n",
    "# Initialize pose analysis classes for left and right arm\n",
    "left_arm_analysis = BicepPoseAnalysis(side=\"left\", stage_down_threshold=STAGE_DOWN_THRESHOLD,\n",
    "                                       stage_up_threshold=STAGE_UP_THRESHOLD,\n",
    "                                       peak_contraction_threshold=PEAK_CONTRACTION_THRESHOLD,\n",
    "                                       loose_upper_arm_angle_threshold=LOOSE_UPPER_ARM_ANGLE_THRESHOLD,\n",
    "                                       visibility_threshold=VISIBILITY_THRESHOLD)\n",
    "\n",
    "right_arm_analysis = BicepPoseAnalysis(side=\"right\", stage_down_threshold=STAGE_DOWN_THRESHOLD,\n",
    "                                        stage_up_threshold=STAGE_UP_THRESHOLD,\n",
    "                                        peak_contraction_threshold=PEAK_CONTRACTION_THRESHOLD,\n",
    "                                        loose_upper_arm_angle_threshold=LOOSE_UPPER_ARM_ANGLE_THRESHOLD,\n",
    "                                        visibility_threshold=VISIBILITY_THRESHOLD)\n",
    "\n",
    "def analyze_pose(video):\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    results_summary = []  # List to store results\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.8, min_tracking_confidence=0.8) as pose:\n",
    "        while cap.isOpened():\n",
    "            ret, image = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Process the image\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(image)\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                \n",
    "                (left_bicep_curl_angle, left_ground_upper_arm_angle) = left_arm_analysis.analyze_pose(landmarks=landmarks, frame=image)\n",
    "                (right_bicep_curl_angle, right_ground_upper_arm_angle) = right_arm_analysis.analyze_pose(landmarks=landmarks, frame=image)\n",
    "\n",
    "                # Use default values if angles are None\n",
    "                left_bicep_curl_angle = left_bicep_curl_angle if left_bicep_curl_angle is not None else 0.0\n",
    "                right_bicep_curl_angle = right_bicep_curl_angle if right_bicep_curl_angle is not None else 0.0\n",
    "\n",
    "                # Prepare results for output as a formatted string\n",
    "                result_summary = (\n",
    "                    f\"Left Angle: {left_bicep_curl_angle:.2f}, Right Angle: {right_bicep_curl_angle:.2f}, \"\n",
    "                    f\"Left Counter: {left_arm_analysis.counter}, Right Counter: {right_arm_analysis.counter}\"\n",
    "                )\n",
    "                \n",
    "                results_summary.append(result_summary)\n",
    "\n",
    "                # Draw landmarks\n",
    "                mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "            # Convert back to BGR for OpenCV\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # You can display the image in the Gradio app\n",
    "            cv2.imshow(\"Pose Analysis\", image)\n",
    "\n",
    "            # Press 'q' to quit the window if needed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Return the summary text as a single formatted string\n",
    "    summary_text = \"\\n\".join(results_summary)\n",
    "    return summary_text\n",
    "\n",
    "# Define Gradio interface\n",
    "iface = gr.Interface(fn=analyze_pose,\n",
    "                     inputs=gr.Video(label=\"Upload a Video for Pose Analysis\"),\n",
    "                     outputs=gr.Textbox(label=\"Analysis Results\"),\n",
    "                     title=\"Bicep Pose Analysis\",\n",
    "                     description=\"Upload a video to analyze bicep curl poses and see the results.\")\n",
    "\n",
    "# Launch Gradio app\n",
    "iface.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
