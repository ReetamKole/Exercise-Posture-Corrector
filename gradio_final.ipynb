{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import gradio as gr\n",
    "import sklearn\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Drawing helpers\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPORTANT_LMS1 = [\n",
    "    \"NOSE\",\n",
    "    \"LEFT_SHOULDER\",\n",
    "    \"RIGHT_SHOULDER\",\n",
    "    \"LEFT_HIP\",\n",
    "    \"RIGHT_HIP\",\n",
    "    \"LEFT_KNEE\",\n",
    "    \"RIGHT_KNEE\",\n",
    "    \"LEFT_ANKLE\",\n",
    "    \"RIGHT_ANKLE\"\n",
    "]\n",
    "\n",
    "headersbc = [\"label\"] # Label column\n",
    "\n",
    "for lm in IMPORTANT_LMS1:\n",
    "    headersbc += [f\"{lm.lower()}_x\", f\"{lm.lower()}_y\", f\"{lm.lower()}_z\", f\"{lm.lower()}_v\"]\n",
    "\n",
    "\n",
    "def extract_important_keypoints(results) -> list:\n",
    "    '''\n",
    "    Extract important keypoints from mediapipe pose detection\n",
    "    '''\n",
    "    landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "    data = []\n",
    "    for lm in IMPORTANT_LMS1:\n",
    "        keypoint = landmarks[mp_pose.PoseLandmark[lm].value]\n",
    "        data.append([keypoint.x, keypoint.y, keypoint.z, keypoint.visibility])\n",
    "    \n",
    "    return np.array(data).flatten().tolist()\n",
    "\n",
    "\n",
    "def rescale_frame(frame, percent=50):\n",
    "    '''\n",
    "    Rescale a frame to a certain percentage compare to its original frame\n",
    "    '''\n",
    "    width = int(frame.shape[1] * percent/ 100)\n",
    "    height = int(frame.shape[0] * percent/ 100)\n",
    "    dim = (width, height)\n",
    "    return cv2.resize(frame, dim, interpolation =cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_frame(frame, percent=50):\n",
    "    '''\n",
    "    Rescale a frame from OpenCV to a certain percentage compare to its original frame\n",
    "    '''\n",
    "    width = int(frame.shape[1] * percent/ 100)\n",
    "    height = int(frame.shape[0] * percent/ 100)\n",
    "    dim = (width, height)\n",
    "    return cv2.resize(frame, dim, interpolation =cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "def save_frame_as_image(frame, message: str = None):\n",
    "    '''\n",
    "    Save a frame as image to display the error\n",
    "    '''\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    if message:\n",
    "        cv2.putText(frame, message, (50, 150), cv2.FONT_HERSHEY_COMPLEX, 0.4, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        \n",
    "    print(\"Saving ...\")\n",
    "    cv2.imwrite(f\"C:/SRM/sem5/ml/project/Exercise-Correction/core/bicep_model/bicep_{now}.jpg\", frame)\n",
    "\n",
    "\n",
    "def calculate_angle(point1: list, point2: list, point3: list) -> float:\n",
    "    '''\n",
    "    Calculate the angle between 3 points\n",
    "    Unit of the angle will be in Degree\n",
    "    '''\n",
    "    point1 = np.array(point1)\n",
    "    point2 = np.array(point2)\n",
    "    point3 = np.array(point3)\n",
    "\n",
    "    # Calculate algo\n",
    "    angleInRad = np.arctan2(point3[1] - point2[1], point3[0] - point2[0]) - np.arctan2(point1[1] - point2[1], point1[0] - point2[0])\n",
    "    angleInDeg = np.abs(angleInRad * 180.0 / np.pi)\n",
    "\n",
    "    angleInDeg = angleInDeg if angleInDeg <= 180 else 360 - angleInDeg\n",
    "    return angleInDeg\n",
    "\n",
    "\n",
    "def extract_important_keypoints(results, IMPORTANT_LMS1: list) -> list:\n",
    "    '''\n",
    "    Extract important keypoints from mediapipe pose detection\n",
    "    '''\n",
    "    landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "    data = []\n",
    "    for lm in IMPORTANT_LMS1:\n",
    "        keypoint = landmarks[mp_pose.PoseLandmark[lm].value]\n",
    "        data.append([keypoint.x, keypoint.y, keypoint.z, keypoint.visibility])\n",
    "    \n",
    "    return np.array(data).flatten().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BicepPoseAnalysis:\n",
    "    def __init__(self, side: str, stage_down_threshold: float, stage_up_threshold: float, peak_contraction_threshold: float, loose_upper_arm_angle_threshold: float, visibility_threshold: float):\n",
    "        # Initialize thresholds\n",
    "        self.stage_down_threshold = stage_down_threshold\n",
    "        self.stage_up_threshold = stage_up_threshold\n",
    "        self.peak_contraction_threshold = peak_contraction_threshold\n",
    "        self.loose_upper_arm_angle_threshold = loose_upper_arm_angle_threshold\n",
    "        self.visibility_threshold = visibility_threshold\n",
    "\n",
    "        self.side = side\n",
    "        self.counter = 0\n",
    "        self.stage = \"down\"\n",
    "        self.is_visible = True\n",
    "        self.detected_errors = {\n",
    "            \"LOOSE_UPPER_ARM\": 0,\n",
    "            \"PEAK_CONTRACTION\": 0,\n",
    "        }\n",
    "\n",
    "        # Params for loose upper arm error detection\n",
    "        self.loose_upper_arm = False\n",
    "\n",
    "        # Params for peak contraction error detection\n",
    "        self.peak_contraction_angle = 1000\n",
    "        self.peak_contraction_frame = None\n",
    "    \n",
    "    def get_joints(self, landmarks) -> bool:\n",
    "        '''\n",
    "        Check for joints' visibility then get joints coordinate\n",
    "        '''\n",
    "        side = self.side.upper()\n",
    "\n",
    "        # Check visibility\n",
    "        joints_visibility = [ landmarks[mp_pose.PoseLandmark[f\"{side}_SHOULDER\"].value].visibility, landmarks[mp_pose.PoseLandmark[f\"{side}_ELBOW\"].value].visibility, landmarks[mp_pose.PoseLandmark[f\"{side}_WRIST\"].value].visibility ]\n",
    "\n",
    "        is_visible = all([ vis > self.visibility_threshold for vis in joints_visibility ])\n",
    "        self.is_visible = is_visible\n",
    "\n",
    "        if not is_visible:\n",
    "            return self.is_visible\n",
    "        \n",
    "        # Get joints' coordinates\n",
    "        self.shoulder = [ landmarks[mp_pose.PoseLandmark[f\"{side}_SHOULDER\"].value].x, landmarks[mp_pose.PoseLandmark[f\"{side}_SHOULDER\"].value].y ]\n",
    "        self.elbow = [ landmarks[mp_pose.PoseLandmark[f\"{side}_ELBOW\"].value].x, landmarks[mp_pose.PoseLandmark[f\"{side}_ELBOW\"].value].y ]\n",
    "        self.wrist = [ landmarks[mp_pose.PoseLandmark[f\"{side}_WRIST\"].value].x, landmarks[mp_pose.PoseLandmark[f\"{side}_WRIST\"].value].y ]\n",
    "\n",
    "        return self.is_visible\n",
    "    \n",
    "    def analyze_pose(self, landmarks, frame):\n",
    "        '''\n",
    "        - Bicep Counter\n",
    "        - Errors Detection\n",
    "        '''\n",
    "        self.get_joints(landmarks)\n",
    "\n",
    "        # Cancel calculation if visibility is poor\n",
    "        if not self.is_visible:\n",
    "            return (None, None)\n",
    "\n",
    "        # * Calculate curl angle for counter\n",
    "        bicep_curl_angle = int(calculate_angle(self.shoulder, self.elbow, self.wrist))\n",
    "        if bicep_curl_angle > self.stage_down_threshold:\n",
    "            self.stage = \"down\"\n",
    "        elif bicep_curl_angle < self.stage_up_threshold and self.stage == \"down\":\n",
    "            self.stage = \"up\"\n",
    "            self.counter += 1\n",
    "        \n",
    "        # * Calculate the angle between the upper arm (shoulder & joint) and the Y axis\n",
    "        shoulder_projection = [ self.shoulder[0], 1 ] # Represent the projection of the shoulder to the X axis\n",
    "        ground_upper_arm_angle = int(calculate_angle(self.elbow, self.shoulder, shoulder_projection))\n",
    "\n",
    "        # * Evaluation for LOOSE UPPER ARM error\n",
    "        if ground_upper_arm_angle > self.loose_upper_arm_angle_threshold:\n",
    "            # Limit the saved frame\n",
    "            if not self.loose_upper_arm:\n",
    "                self.loose_upper_arm = True\n",
    "                # save_frame_as_image(frame, f\"Loose upper arm: {ground_upper_arm_angle}\")\n",
    "                self.detected_errors[\"LOOSE_UPPER_ARM\"] += 1\n",
    "        else:\n",
    "            self.loose_upper_arm = False\n",
    "        \n",
    "        # * Evaluate PEAK CONTRACTION error\n",
    "        if self.stage == \"up\" and bicep_curl_angle < self.peak_contraction_angle:\n",
    "            # Save peaked contraction every rep\n",
    "            self.peak_contraction_angle = bicep_curl_angle\n",
    "            self.peak_contraction_frame = frame\n",
    "            \n",
    "        elif self.stage == \"down\":\n",
    "            # * Evaluate if the peak is higher than the threshold if True, marked as an error then saved that frame\n",
    "            if self.peak_contraction_angle != 1000 and self.peak_contraction_angle >= self.peak_contraction_threshold:\n",
    "                # save_frame_as_image(self.peak_contraction_frame, f\"{self.side} - Peak Contraction: {self.peak_contraction_angle}\")\n",
    "                self.detected_errors[\"PEAK_CONTRACTION\"] += 1\n",
    "            \n",
    "            # Reset params\n",
    "            self.peak_contraction_angle = 1000\n",
    "            self.peak_contraction_frame = None\n",
    "        \n",
    "        return (bicep_curl_angle, ground_upper_arm_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_lnd = [\n",
    "    \"NOSE\",\n",
    "    \"LEFT_SHOULDER\",\n",
    "    \"RIGHT_SHOULDER\",\n",
    "    \"LEFT_HIP\",\n",
    "    \"RIGHT_HIP\",\n",
    "    \"LEFT_KNEE\",\n",
    "    \"RIGHT_KNEE\",\n",
    "    \"LEFT_ANKLE\",\n",
    "    \"RIGHT_ANKLE\"\n",
    "]\n",
    "\n",
    "headersquat = [\"label\"] # Label column\n",
    "\n",
    "for lm in imp_lnd:\n",
    "    headersquat += [f\"{lm.lower()}_x\", f\"{lm.lower()}_y\", f\"{lm.lower()}_z\", f\"{lm.lower()}_v\"]\n",
    "\n",
    "\n",
    "def extract_important_keypoints_sq(results) -> list:\n",
    "    '''\n",
    "    Extract important keypoints from mediapipe pose detection\n",
    "    '''\n",
    "    landmark = results.pose_landmarks.landmark\n",
    "\n",
    "    data = []\n",
    "    for lm in imp_lnd:\n",
    "        keypoint = landmark[mp_pose.PoseLandmark[lm].value]\n",
    "        data.append([keypoint.x, keypoint.y, keypoint.z, keypoint.visibility])\n",
    "    \n",
    "    return np.array(data).flatten().tolist()\n",
    "\n",
    "\n",
    "def rescale_frame(frame, percent=50):\n",
    "    '''\n",
    "    Rescale a frame to a certain percentage compare to its original frame\n",
    "    '''\n",
    "    width = int(frame.shape[1] * percent/ 100)\n",
    "    height = int(frame.shape[0] * percent/ 100)\n",
    "    dim = (width, height)\n",
    "    return cv2.resize(frame, dim, interpolation =cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "\n",
    "def calculate_distance(pointX, pointY) -> float:\n",
    "    '''\n",
    "    Calculate a distance between 2 points\n",
    "    '''\n",
    "\n",
    "    x1, y1 = pointX\n",
    "    x2, y2 = pointY\n",
    "\n",
    "    return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "\n",
    "\n",
    "def analyze_foot_knee_placement(results, stage: str, foot_shoulder_ratio_thresholds: list, knee_foot_ratio_thresholds: dict, visibility_threshold: int) -> dict:\n",
    "    '''\n",
    "    Calculate the ratio between the foot and shoulder for FOOT PLACEMENT analysis\n",
    "    \n",
    "    Calculate the ratio between the knee and foot for KNEE PLACEMENT analysis\n",
    "\n",
    "    Return result explanation:\n",
    "        -1: Unknown result due to poor visibility\n",
    "        0: Correct knee placement\n",
    "        1: Placement too tight\n",
    "        2: Placement too wide\n",
    "    '''\n",
    "    analyzed_results = {\n",
    "        \"foot_placement\": -1,\n",
    "        \"knee_placement\": -1,\n",
    "    }\n",
    "\n",
    "    landmark = results.pose_landmarks.landmark\n",
    "\n",
    "    # * Visibility check of important landmark for foot placement analysis\n",
    "    left_foot_index_vis = landmark[mp_pose.PoseLandmark.LEFT_FOOT_INDEX.value].visibility\n",
    "    right_foot_index_vis = landmark[mp_pose.PoseLandmark.RIGHT_FOOT_INDEX.value].visibility\n",
    "\n",
    "    left_knee_vis = landmark[mp_pose.PoseLandmark.LEFT_KNEE.value].visibility\n",
    "    right_knee_vis = landmark[mp_pose.PoseLandmark.RIGHT_KNEE.value].visibility\n",
    "\n",
    "    # If visibility of any keypoints is low cancel the analysis\n",
    "    if (left_foot_index_vis < visibility_threshold or right_foot_index_vis < visibility_threshold or left_knee_vis < visibility_threshold or right_knee_vis < visibility_threshold):\n",
    "        return analyzed_results\n",
    "    \n",
    "    # * Calculate shoulder width\n",
    "    left_shoulder = [landmark[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmark[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "    right_shoulder = [landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "    shoulder_width = calculate_distance(left_shoulder, right_shoulder)\n",
    "\n",
    "    # * Calculate 2-foot width\n",
    "    left_foot_index = [landmark[mp_pose.PoseLandmark.LEFT_FOOT_INDEX.value].x, landmark[mp_pose.PoseLandmark.LEFT_FOOT_INDEX.value].y]\n",
    "    right_foot_index = [landmark[mp_pose.PoseLandmark.RIGHT_FOOT_INDEX.value].x, landmark[mp_pose.PoseLandmark.RIGHT_FOOT_INDEX.value].y]\n",
    "    foot_width = calculate_distance(left_foot_index, right_foot_index)\n",
    "\n",
    "    # * Calculate foot and shoulder ratio\n",
    "    foot_shoulder_ratio = round(foot_width / shoulder_width, 1)\n",
    "\n",
    "    # * Analyze FOOT PLACEMENT\n",
    "    min_ratio_foot_shoulder, max_ratio_foot_shoulder = foot_shoulder_ratio_thresholds\n",
    "    if min_ratio_foot_shoulder <= foot_shoulder_ratio <= max_ratio_foot_shoulder:\n",
    "        analyzed_results[\"foot_placement\"] = 0\n",
    "    elif foot_shoulder_ratio < min_ratio_foot_shoulder:\n",
    "        analyzed_results[\"foot_placement\"] = 1\n",
    "    elif foot_shoulder_ratio > max_ratio_foot_shoulder:\n",
    "        analyzed_results[\"foot_placement\"] = 2\n",
    "    \n",
    "    # * Visibility check of important landmark for knee placement analysis\n",
    "    left_knee_vis = landmark[mp_pose.PoseLandmark.LEFT_KNEE.value].visibility\n",
    "    right_knee_vis = landmark[mp_pose.PoseLandmark.RIGHT_KNEE.value].visibility\n",
    "\n",
    "    # If visibility of any keypoints is low cancel the analysis\n",
    "    if (left_knee_vis < visibility_threshold or right_knee_vis < visibility_threshold):\n",
    "        print(\"Cannot see foot\")\n",
    "        return analyzed_results\n",
    "\n",
    "    # * Calculate 2 knee width\n",
    "    left_knee = [landmark[mp_pose.PoseLandmark.LEFT_KNEE.value].x, landmark[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "    right_knee = [landmark[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, landmark[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "    knee_width = calculate_distance(left_knee, right_knee)\n",
    "\n",
    "    # * Calculate foot and shoulder ratio\n",
    "    knee_foot_ratio = round(knee_width / foot_width, 1)\n",
    "\n",
    "    # * Analyze KNEE placement\n",
    "    up_min_ratio_knee_foot, up_max_ratio_knee_foot = knee_foot_ratio_thresholds.get(\"up\")\n",
    "    middle_min_ratio_knee_foot, middle_max_ratio_knee_foot = knee_foot_ratio_thresholds.get(\"middle\")\n",
    "    down_min_ratio_knee_foot, down_max_ratio_knee_foot = knee_foot_ratio_thresholds.get(\"down\")\n",
    "\n",
    "    if stage == \"up\":\n",
    "        if up_min_ratio_knee_foot <= knee_foot_ratio <= up_max_ratio_knee_foot:\n",
    "            analyzed_results[\"knee_placement\"] = 0\n",
    "        elif knee_foot_ratio < up_min_ratio_knee_foot:\n",
    "            analyzed_results[\"knee_placement\"] = 1\n",
    "        elif knee_foot_ratio > up_max_ratio_knee_foot:\n",
    "            analyzed_results[\"knee_placement\"] = 2\n",
    "    elif stage == \"middle\":\n",
    "        if middle_min_ratio_knee_foot <= knee_foot_ratio <= middle_max_ratio_knee_foot:\n",
    "            analyzed_results[\"knee_placement\"] = 0\n",
    "        elif knee_foot_ratio < middle_min_ratio_knee_foot:\n",
    "            analyzed_results[\"knee_placement\"] = 1\n",
    "        elif knee_foot_ratio > middle_max_ratio_knee_foot:\n",
    "            analyzed_results[\"knee_placement\"] = 2\n",
    "    elif stage == \"down\":\n",
    "        if down_min_ratio_knee_foot <= knee_foot_ratio <= down_max_ratio_knee_foot:\n",
    "            analyzed_results[\"knee_placement\"] = 0\n",
    "        elif knee_foot_ratio < down_min_ratio_knee_foot:\n",
    "            analyzed_results[\"knee_placement\"] = 1\n",
    "        elif knee_foot_ratio > down_max_ratio_knee_foot:\n",
    "            analyzed_results[\"knee_placement\"] = 2\n",
    "    \n",
    "    return analyzed_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/SRM/sem5/ml/project/ml proj/squat_model/model/LR_model.pkl\", \"rb\") as f:\n",
    "    count_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_pu = [\n",
    "    \"NOSE\",\n",
    "    \"LEFT_EYE_INNER\",\n",
    "    \"LEFT_EYE\",\n",
    "    \"LEFT_EYE_OUTER\",\n",
    "    \"RIGHT_EYE_INNER\",\n",
    "    \"RIGHT_EYE\",\n",
    "    \"RIGHT_EYE_OUTER\",\n",
    "    \"LEFT_EAR\",\n",
    "    \"RIGHT_EAR\",\n",
    "    \"MOUTH_LEFT\",\n",
    "    \"MOUTH_RIGHT\",\n",
    "    \"LEFT_SHOULDER\",\n",
    "    \"RIGHT_SHOULDER\",\n",
    "    \"LEFT_ELBOW\",\n",
    "    \"RIGHT_ELBOW\",\n",
    "    \"LEFT_WRIST\",\n",
    "    \"RIGHT_WRIST\",\n",
    "    \"LEFT_PINKY\",\n",
    "    \"RIGHT_PINKY\",\n",
    "    \"LEFT_INDEX\",\n",
    "    \"RIGHT_INDEX\",\n",
    "    \"LEFT_THUMB\",\n",
    "    \"RIGHT_THUMB\",\n",
    "    \"LEFT_HIP\",\n",
    "    \"RIGHT_HIP\",\n",
    "    \"LEFT_KNEE\",\n",
    "    \"RIGHT_KNEE\",\n",
    "    \"LEFT_ANKLE\",\n",
    "    \"RIGHT_ANKLE\",\n",
    "    \"LEFT_HEEL\",\n",
    "    \"RIGHT_HEEL\",\n",
    "    \"LEFT_FOOT_INDEX\",\n",
    "    \"RIGHT_FOOT_INDEX\"\n",
    "]\n",
    "headerspu = [\"label\"]\n",
    "for lm in imp_pu:\n",
    "    headerspu += [f\"{lm.lower()}_x\", f\"{lm.lower()}_y\", f\"{lm.lower()}_z\", f\"{lm.lower()}_v\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_important_keypoints_pu(results) -> list:\n",
    "    '''\n",
    "    Extract important keypoints from mediapipe pose detection\n",
    "    '''\n",
    "    landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "    data = []\n",
    "    for lm in imp_pu:\n",
    "        keypoint = landmarks[mp_pose.PoseLandmark[lm].value]\n",
    "        data.append([keypoint.x, keypoint.y, keypoint.z, keypoint.visibility])\n",
    "    \n",
    "    return np.array(data).flatten().tolist()\n",
    "\n",
    "\n",
    "def rescale_frame(frame, percent=50):\n",
    "    '''\n",
    "    Rescale a frame to a certain percentage compare to its original frame\n",
    "    '''\n",
    "    width = int(frame.shape[1] * percent/ 100)\n",
    "    height = int(frame.shape[0] * percent/ 100)\n",
    "    dim = (width, height)\n",
    "    return cv2.resize(frame, dim, interpolation =cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "with open(\"C:/SRM/sem5/ml/project/ml proj/pushup_model/pickles/RF_model.pkl\", \"rb\") as f:\n",
    "    sklearn_model_pu = pickle.load(f)\n",
    "\n",
    "# Load input scaler\n",
    "with open(\"C:/SRM/sem5/ml/project/ml proj/pushup_model/pickles/input_scaler.pkl\", \"rb\") as f2:\n",
    "    input_scaler_pu = pickle.load(f2)\n",
    "\n",
    "# Transform prediction into class\n",
    "def get_class(prediction: float) -> str:\n",
    "    return {\n",
    "        0: \"C\",\n",
    "        1: \"H\",\n",
    "        2: \"L\",\n",
    "    }.get(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "with open(\"C:/SRM/sem5/ml/project/ml proj/pushup_model/pickles/pushup_dp.pkl\", \"rb\") as f:\n",
    "    DL_model_pu = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine important landmarks for plank\n",
    "IMP_LMS = [\n",
    "    \"NOSE\",\n",
    "    \"LEFT_SHOULDER\",\n",
    "    \"RIGHT_SHOULDER\",\n",
    "    \"LEFT_ELBOW\",\n",
    "    \"RIGHT_ELBOW\",\n",
    "    \"LEFT_WRIST\",\n",
    "    \"RIGHT_WRIST\",\n",
    "    \"LEFT_HIP\",\n",
    "    \"RIGHT_HIP\",\n",
    "    \"LEFT_KNEE\",\n",
    "    \"RIGHT_KNEE\",\n",
    "    \"LEFT_ANKLE\",\n",
    "    \"RIGHT_ANKLE\",\n",
    "    \"LEFT_HEEL\",\n",
    "    \"RIGHT_HEEL\",\n",
    "    \"LEFT_FOOT_INDEX\",\n",
    "    \"RIGHT_FOOT_INDEX\",\n",
    "]\n",
    "\n",
    "# Generate all columns of the data frame\n",
    "\n",
    "headersplank = [\"label\"] # Label column\n",
    "\n",
    "for lm in IMP_LMS:\n",
    "    headersplank += [f\"{lm.lower()}_x\", f\"{lm.lower()}_y\", f\"{lm.lower()}_z\", f\"{lm.lower()}_v\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_important_keypoints_pl(results) -> list:\n",
    "    '''\n",
    "    Extract important keypoints from mediapipe pose detection\n",
    "    '''\n",
    "    landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "    data = []\n",
    "    for lm in IMP_LMS:\n",
    "        keypoint = landmarks[mp_pose.PoseLandmark[lm].value]\n",
    "        data.append([keypoint.x, keypoint.y, keypoint.z, keypoint.visibility])\n",
    "    \n",
    "    return np.array(data).flatten().tolist()\n",
    "\n",
    "\n",
    "def rescale_frame(frame, percent=50):\n",
    "    '''\n",
    "    Rescale a frame to a certain percentage compare to its original frame\n",
    "    '''\n",
    "    width = int(frame.shape[1] * percent/ 100)\n",
    "    height = int(frame.shape[0] * percent/ 100)\n",
    "    dim = (width, height)\n",
    "    return cv2.resize(frame, dim, interpolation =cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "with open(\"C:/SRM/sem5/ml/project/ml proj/plank_model/model/LR_model.pkl\", \"rb\") as f:\n",
    "    sklearn_model_pl = pickle.load(f)\n",
    "\n",
    "# Load input scaler\n",
    "with open(\"C:/SRM/sem5/ml/project/ml proj/plank_model/model/input_scaler.pkl\", \"rb\") as f2:\n",
    "    input_scaler_pl = pickle.load(f2)\n",
    "\n",
    "# Transform prediction into class\n",
    "def get_class_plank(prediction: float) -> str:\n",
    "    return {\n",
    "        0: \"C\",\n",
    "        1: \"H\",\n",
    "        2: \"L\",\n",
    "    }.get(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize MediaPipe pose and drawing utilities\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Define constants for Bicep analysis\n",
    "VISIBILITY_THRESHOLD_BICEP = 0.65\n",
    "STAGE_UP_THRESHOLD_BICEP = 90\n",
    "STAGE_DOWN_THRESHOLD_BICEP = 120\n",
    "PEAK_CONTRACTION_THRESHOLD_BICEP = 60\n",
    "LOOSE_UPPER_ARM_ANGLE_THRESHOLD_BICEP = 40\n",
    "\n",
    "# Bicep Pose Analysis class initialization (you should have this class defined)\n",
    "left_arm_analysis = BicepPoseAnalysis(side=\"left\", stage_down_threshold=STAGE_DOWN_THRESHOLD_BICEP,\n",
    "                                       stage_up_threshold=STAGE_UP_THRESHOLD_BICEP,\n",
    "                                       peak_contraction_threshold=PEAK_CONTRACTION_THRESHOLD_BICEP,\n",
    "                                       loose_upper_arm_angle_threshold=LOOSE_UPPER_ARM_ANGLE_THRESHOLD_BICEP,\n",
    "                                       visibility_threshold=VISIBILITY_THRESHOLD_BICEP)\n",
    "\n",
    "right_arm_analysis = BicepPoseAnalysis(side=\"right\", stage_down_threshold=STAGE_DOWN_THRESHOLD_BICEP,\n",
    "                                        stage_up_threshold=STAGE_UP_THRESHOLD_BICEP,\n",
    "                                        peak_contraction_threshold=PEAK_CONTRACTION_THRESHOLD_BICEP,\n",
    "                                        loose_upper_arm_angle_threshold=LOOSE_UPPER_ARM_ANGLE_THRESHOLD_BICEP,\n",
    "                                        visibility_threshold=VISIBILITY_THRESHOLD_BICEP)\n",
    "\n",
    "# Define the analyze_pose function for Bicep Curl Analysis\n",
    "def analyze_bicep_pose(video):\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    results_summary = []\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.8, min_tracking_confidence=0.8) as pose:\n",
    "        while cap.isOpened():\n",
    "            ret, image = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(image)\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                (left_bicep_curl_angle, _) = left_arm_analysis.analyze_pose(landmarks=landmarks, frame=image)\n",
    "                (right_bicep_curl_angle, _) = right_arm_analysis.analyze_pose(landmarks=landmarks, frame=image)\n",
    "\n",
    "                left_bicep_curl_angle = left_bicep_curl_angle if left_bicep_curl_angle is not None else 0.0\n",
    "                right_bicep_curl_angle = right_bicep_curl_angle if right_bicep_curl_angle is not None else 0.0\n",
    "\n",
    "                result_summary = (\n",
    "                    f\"Left Angle: {left_bicep_curl_angle:.2f}, Right Angle: {right_bicep_curl_angle:.2f}, \"\n",
    "                    f\"Left Counter: {left_arm_analysis.counter}, Right Counter: {right_arm_analysis.counter}\"\n",
    "                )\n",
    "                results_summary.append(result_summary)\n",
    "\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imshow(\"Bicep Pose Analysis\", image)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    summary_text = \"\\n\".join(results_summary)\n",
    "    return summary_text\n",
    "\n",
    "# Define constants for Squat analysis\n",
    "PREDICTION_PROB_THRESHOLD_SQUAT = 0.7\n",
    "VISIBILITY_THRESHOLD_SQUAT = 0.6\n",
    "FOOT_SHOULDER_RATIO_THRESHOLDS = [1.2, 2.8]\n",
    "KNEE_FOOT_RATIO_THRESHOLDS = {\n",
    "    \"up\": [0.5, 1.0],\n",
    "    \"middle\": [0.7, 1.0],\n",
    "    \"down\": [0.7, 1.1],\n",
    "}\n",
    "\n",
    "# Define the process_video function for Squat Analysis\n",
    "def analyze_squat_pose(video):\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    counter = 0\n",
    "    current_stage = \"\"\n",
    "    results_summary = []\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        while cap.isOpened():\n",
    "            ret, image = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(image)\n",
    "\n",
    "            if not results.pose_landmarks:\n",
    "                continue\n",
    "\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "            try:\n",
    "                row = extract_important_keypoints_sq(results)\n",
    "                X = pd.DataFrame([row], columns=headersquat[1:])\n",
    "\n",
    "                predicted_class = count_model.predict(X)[0]\n",
    "                predicted_class = \"down\" if predicted_class == 0 else \"up\"\n",
    "                prediction_probabilities = count_model.predict_proba(X)[0]\n",
    "                prediction_probability = round(prediction_probabilities[prediction_probabilities.argmax()], 2)\n",
    "\n",
    "                if predicted_class == \"down\" and prediction_probability >= PREDICTION_PROB_THRESHOLD_SQUAT:\n",
    "                    current_stage = \"down\"\n",
    "                elif current_stage == \"down\" and predicted_class == \"up\" and prediction_probability >= PREDICTION_PROB_THRESHOLD_SQUAT: \n",
    "                    current_stage = \"up\"\n",
    "                    counter += 1\n",
    "\n",
    "                analyzed_results = analyze_foot_knee_placement(results=results, stage=current_stage, \n",
    "                                                                foot_shoulder_ratio_thresholds=FOOT_SHOULDER_RATIO_THRESHOLDS, \n",
    "                                                                knee_foot_ratio_thresholds=KNEE_FOOT_RATIO_THRESHOLDS, \n",
    "                                                                visibility_threshold=VISIBILITY_THRESHOLD_SQUAT)\n",
    "\n",
    "                foot_placement_evaluation = analyzed_results[\"foot_placement\"]\n",
    "                knee_placement_evaluation = analyzed_results[\"knee_placement\"]\n",
    "                \n",
    "                foot_placement = [\"Correct\", \"Too tight\", \"Too wide\", \"UNK\"][foot_placement_evaluation] if foot_placement_evaluation != -1 else \"UNK\"\n",
    "                knee_placement = [\"Correct\", \"Too tight\", \"Too wide\", \"UNK\"][knee_placement_evaluation] if knee_placement_evaluation != -1 else \"UNK\"\n",
    "\n",
    "                results_summary.append({\n",
    "                    \"counter\": counter,\n",
    "                    \"stage\": current_stage,\n",
    "                    \"prediction_probability\": prediction_probability,\n",
    "                    \"foot_placement\": foot_placement,\n",
    "                    \"knee_placement\": knee_placement\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "        cap.release()\n",
    "    \n",
    "    summary_text = \"\\n\".join([f\"Probability: {result['prediction_probability']}, \"\n",
    "                              f\"Foot Placement: {result['foot_placement']}, Knee Placement: {result['knee_placement']}\" \n",
    "                              for result in results_summary])\n",
    "    \n",
    "    return summary_text\n",
    "\n",
    "#pushup analysis\n",
    "PREDICTION_PROB_THRESHOLD_PUSHUP=0.6\n",
    "def analyze_pushup_pose(video):\n",
    "    cap = cv2.VideoCapture(video)  \n",
    "    current_stage = \"\"\n",
    "    results_summary = []\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        while cap.isOpened():\n",
    "            ret, image = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Recolor image from BGR to RGB for mediapipe\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(image)\n",
    "\n",
    "            if not results.pose_landmarks:\n",
    "                results_summary.append(\"No human found\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                row = extract_important_keypoints_pu(results)\n",
    "                X = pd.DataFrame([row], columns=headerspu[1:])\n",
    "                X = pd.DataFrame(input_scaler_pu.transform(X))\n",
    "\n",
    "                # Make prediction\n",
    "                predictions_pu = DL_model_pu.predict(X)\n",
    "                predicted_class_pu = np.argmax(predictions_pu)  # Get the index of the highest predicted probability\n",
    "                prediction_probability_pu = np.max(predictions_pu)  # Get the maximum probability\n",
    "\n",
    "                if predicted_class_pu == 0 and prediction_probability_pu >= PREDICTION_PROB_THRESHOLD_PUSHUP:\n",
    "                    current_stage_pu = \"Correct\"\n",
    "                elif predicted_class_pu == 2 and prediction_probability_pu >= PREDICTION_PROB_THRESHOLD_PUSHUP:\n",
    "                    current_stage_pu = \"Low back\"\n",
    "                elif predicted_class_pu == 1 and prediction_probability_pu >= PREDICTION_PROB_THRESHOLD_PUSHUP:\n",
    "                    current_stage_pu = \"High back\"\n",
    "                else:\n",
    "                    current_stage_pu = \"Unknown\"\n",
    "\n",
    "                results_summary.append(f\"Stage: {current_stage_pu}, Probability: {prediction_probability_pu:.2f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                results_summary.append(f\"Error: {e}\")\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "    return \"\\n\".join(results_summary)\n",
    "\n",
    "#PLANK ANALYSIS\n",
    "PREDICTION_PROB_THRESHOLD_PLANK=0.6\n",
    "def analyze_plank_pose(video):\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    prediction_probability_threshold = PREDICTION_PROB_THRESHOLD_PLANK\n",
    "    output_text = []\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        while cap.isOpened():\n",
    "            ret, image = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Convert BGR to RGB\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Process the frame with MediaPipe\n",
    "            results = pose.process(image)\n",
    "\n",
    "            if not results.pose_landmarks:\n",
    "                output_text.append(\"No human found\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Extract keypoints and make predictions\n",
    "                row = extract_important_keypoints_pl(results)\n",
    "                X = pd.DataFrame([row], columns=headersplank[1:])\n",
    "                X = pd.DataFrame(input_scaler_pl.transform(X))\n",
    "\n",
    "                # Prediction\n",
    "                predicted_class = sklearn_model_pl.predict(X)[0]\n",
    "                predicted_class = get_class(predicted_class)\n",
    "                prediction_probability = sklearn_model_pl.predict_proba(X)[0]\n",
    "                prob_value = round(prediction_probability[np.argmax(prediction_probability)], 2)\n",
    "\n",
    "                if predicted_class == \"C\" and prediction_probability[prediction_probability.argmax()] >= prediction_probability_threshold:\n",
    "                    current_stage = \"Correct\"\n",
    "                elif predicted_class == \"L\" and prediction_probability[prediction_probability.argmax()] >= prediction_probability_threshold: \n",
    "                    current_stage = \"Low back\"\n",
    "                elif predicted_class == \"H\" and prediction_probability[prediction_probability.argmax()] >= prediction_probability_threshold: \n",
    "                    current_stage = \"High back\"\n",
    "                else:\n",
    "                    current_stage = \"Unknown\"\n",
    "\n",
    "                # Append result\n",
    "                output_text.append(f\"Stage: {current_stage}, Probability: {prob_value}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                output_text.append(f\"Error: {e}\")\n",
    "        \n",
    "        cap.release()\n",
    "\n",
    "    return \"\\n\".join(output_text)\n",
    "\n",
    "\n",
    "# Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Bicep, Squat, Pushup, and Plank Pose Analysis\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"## Bicep Curl Analysis\")\n",
    "            bicep_input = gr.Video(label=\"Upload a Video for Bicep Curl Analysis\")\n",
    "            bicep_output = gr.Textbox(label=\"Bicep Analysis Results\", interactive=False)\n",
    "            bicep_button = gr.Button(\"Analyze Bicep Curl\")\n",
    "            bicep_button.click(analyze_bicep_pose, inputs=bicep_input, outputs=bicep_output)\n",
    "\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"## Squat Analysis\")\n",
    "            squat_input = gr.Video(label=\"Upload a Video for Squat Analysis\")\n",
    "            squat_output = gr.Textbox(label=\"Squat Analysis Results\", interactive=False)\n",
    "            squat_button = gr.Button(\"Analyze Squat\")\n",
    "            squat_button.click(analyze_squat_pose, inputs=squat_input, outputs=squat_output)\n",
    "\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"## Pushup Analysis\")\n",
    "            pushup_input = gr.Video(label=\"Upload a Video for Pushup Analysis\")\n",
    "            pushup_output = gr.Textbox(label=\"Pushup Analysis Results\", interactive=False)\n",
    "            pushup_button = gr.Button(\"Analyze Pushup\")\n",
    "            pushup_button.click(analyze_pushup_pose, inputs=pushup_input, outputs=pushup_output)\n",
    "\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"## Plank Analysis\")\n",
    "            plank_input = gr.Video(label=\"Upload a Video for Plank Analysis\")\n",
    "            plank_output = gr.Textbox(label=\"Plank Analysis Results\", interactive=False)\n",
    "            plank_button = gr.Button(\"Analyze Plank\")\n",
    "            plank_button.click(analyze_plank_pose, inputs=plank_input, outputs=plank_output)\n",
    "\n",
    "# Launch Gradio app\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
